{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0286b616",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2581670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import sqlite3\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from tqdm import tqdm\n",
    "from sqlalchemy import create_engine, text\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import silhouette_score\n",
    "import urllib.request\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ All dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283abb78",
   "metadata": {},
   "source": [
    "## Configuring Directories and Processing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a31b3abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration initialized\n",
      "  - Data directory: poc_data\n",
      "  - Chunk size: 100,000\n",
      "  - Total samples: 2,000,000\n",
      "  - Initial K clusters: 6\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    # Directories\n",
    "    DATA_DIR = Path(\"./poc_data\")\n",
    "    LOG_DIR = DATA_DIR / \"event_log\"\n",
    "    OFFLINE_FEATURE_DIR = DATA_DIR / \"offline_features\"\n",
    "    MODEL_DIR = DATA_DIR / \"models\"\n",
    "    VECTOR_DB_DIR = DATA_DIR / \"vector_db\"\n",
    "    \n",
    "    # Files\n",
    "    SQLITE_DB = DATA_DIR / \"feature_store.db\"\n",
    "    HISTORICAL_DATA = DATA_DIR / \"historical_full.parquet\"\n",
    "    AI_METRICS_FILE = DATA_DIR / \"ai_metrics.parquet\"\n",
    "    \n",
    "    # Processing parameters\n",
    "    CHUNK_SIZE = 100000\n",
    "    TOTAL_SAMPLES = 2000000  # Reduced for 16GB RAM\n",
    "    RANDOM_SEED = 42\n",
    "    INITIAL_K = 6\n",
    "    REPROCESS_K = 8\n",
    "    EMBEDDING_DIM = 128  # Reduced dimensionality for memory efficiency\n",
    "    \n",
    "    # NYC Taxi Data URL\n",
    "    TAXI_DATA_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "print(\"✓ Configuration initialized\")\n",
    "print(f\"  - Data directory: {config.DATA_DIR}\")\n",
    "print(f\"  - Chunk size: {config.CHUNK_SIZE:,}\")\n",
    "print(f\"  - Total samples: {config.TOTAL_SAMPLES:,}\")\n",
    "print(f\"  - Initial K clusters: {config.INITIAL_K}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b823fe30",
   "metadata": {},
   "source": [
    "## Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22470434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All directories created\n",
      "\n",
      "Directory structure created:\n",
      "  📁 poc_data\n",
      "  📁 poc_data\\event_log\n",
      "  📁 poc_data\\offline_features\n",
      "  📁 poc_data\\models\n",
      "  📁 poc_data\\vector_db\n"
     ]
    }
   ],
   "source": [
    "def setup_directories():\n",
    "    \"\"\"Create all necessary directories\"\"\"\n",
    "    directories = [\n",
    "        config.DATA_DIR, config.LOG_DIR, config.OFFLINE_FEATURE_DIR,\n",
    "        config.MODEL_DIR, config.VECTOR_DB_DIR\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"✓ All directories created\")\n",
    "\n",
    "# Execute setup\n",
    "setup_directories()\n",
    "\n",
    "# Display directory structure\n",
    "print(\"\\nDirectory structure created:\")\n",
    "for directory in [config.DATA_DIR, config.LOG_DIR, config.OFFLINE_FEATURE_DIR, config.MODEL_DIR, config.VECTOR_DB_DIR]:\n",
    "    print(f\"{directory}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c946b",
   "metadata": {},
   "source": [
    "## Feature Engineering & Loading Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcfc4db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NYC taxi data...\n",
      "  ✓ Loading cached data...\n",
      "  Original dataset size: 3,066,766 records\n",
      "  Dataset size after dropping duplicates: 3,066,766 records\n",
      "  Dataset size after dropping NaNs: 2,995,023 records\n",
      "  Sampled to: 2,000,000 records\n",
      "\n",
      "✓ Data preprocessing complete!\n",
      "  Final dataset size: 1,956,509 records\n",
      "  Number of features: 8\n",
      "  Feature columns: ['trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Memory usage: 176.20 MB\n",
      "\n",
      "Sample data:\n",
      "        trip_id  trip_distance  total_amount  pickup_hour  pickup_day  \\\n",
      "2858416  trip_0       0.400000     10.800000           15           0   \n",
      "873744   trip_1       0.600000     12.950000           21           1   \n",
      "2361819  trip_2       2.100000     29.900000           18           2   \n",
      "1703896  trip_3      16.299999     96.650002            9           3   \n",
      "475582   trip_4       1.740000     22.000000           17           4   \n",
      "\n",
      "         trip_duration  passenger_count  pickup_location  dropoff_location  \n",
      "2858416       4.266667              1.0                7                 7  \n",
      "873744        5.066667              1.0                6                 6  \n",
      "2361819      22.549999              1.0                3                 6  \n",
      "1703896      36.849998              3.0                2                 2  \n",
      "475582       14.500000              1.0                3                 1  \n"
     ]
    }
   ],
   "source": [
    "def create_taxi_features(df):\n",
    "    \"\"\"Create features suitable for clustering from taxi data\"\"\"\n",
    "    # Filter reasonable values\n",
    "    df = df[\n",
    "        (df.trip_distance > 0) & \n",
    "        (df.trip_distance < 100) &\n",
    "        (df.total_amount > 0) &\n",
    "        (df.total_amount < 200)\n",
    "    ].copy()\n",
    "    \n",
    "    # Create time-based features\n",
    "    df['pickup_hour'] = df.tpep_pickup_datetime.dt.hour\n",
    "    df['pickup_day'] = df.tpep_pickup_datetime.dt.dayofweek\n",
    "    df['trip_duration'] = (df.tpep_dropoff_datetime - df.tpep_pickup_datetime).dt.total_seconds() / 60\n",
    "    \n",
    "    # Filter out unrealistic durations\n",
    "    df = df[(df.trip_duration > 0) & (df.trip_duration < 180)]\n",
    "    \n",
    "    # Create geospatial features\n",
    "    df['pickup_location'] = (df['PULocationID'] % 10).astype('category').cat.codes\n",
    "    df['dropoff_location'] = (df['DOLocationID'] % 10).astype('category').cat.codes\n",
    "    \n",
    "    # Select features for clustering\n",
    "    feature_cols = [\n",
    "        'trip_distance', 'total_amount', 'pickup_hour', \n",
    "        'pickup_day', 'trip_duration', 'passenger_count',\n",
    "        'pickup_location', 'dropoff_location'\n",
    "    ]\n",
    "    \n",
    "    # Add unique ID\n",
    "    df['trip_id'] = [f\"trip_{i}\" for i in range(len(df))]\n",
    "    \n",
    "    return df[['trip_id'] + feature_cols], feature_cols\n",
    "\n",
    "def optimize_dataframe(df):\n",
    "    \"\"\"Reduce memory usage of DataFrame\"\"\"\n",
    "    # Downcast numeric types\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    # Convert objects to category where possible\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if df[col].nunique() / len(df) < 0.5:  # If cardinality < 50%\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess NYC taxi data\"\"\"\n",
    "    print(\"Loading NYC taxi data...\")\n",
    "    \n",
    "    # Download data if not exists\n",
    "    local_data_path = config.DATA_DIR / \"nyc_taxi_data.parquet\"\n",
    "    \n",
    "    if not local_data_path.exists():\n",
    "        print(\"  Downloading data from NYC OpenData...\")\n",
    "        df = pd.read_parquet(config.TAXI_DATA_URL)\n",
    "        df.to_parquet(local_data_path)\n",
    "        print(\"  ✓ Data downloaded and cached locally\")\n",
    "    else:\n",
    "        print(\"  ✓ Loading cached data...\")\n",
    "        df = pd.read_parquet(local_data_path)\n",
    "    \n",
    "    print(f\"  Original dataset size: {len(df):,} records\")\n",
    "    \n",
    "    # Drop duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"  Dataset size after dropping duplicates: {len(df):,} records\")\n",
    "    \n",
    "    # Drop rows with NaNs\n",
    "    df = df.dropna()\n",
    "    print(f\"  Dataset size after dropping NaNs: {len(df):,} records\")\n",
    "    \n",
    "    # Sample to fit memory constraints\n",
    "    if len(df) > config.TOTAL_SAMPLES:\n",
    "        df = df.sample(config.TOTAL_SAMPLES, random_state=config.RANDOM_SEED)\n",
    "        print(f\"  Sampled to: {len(df):,} records\")\n",
    "    \n",
    "    # Create features for clustering\n",
    "    df, feature_cols = create_taxi_features(df)\n",
    "    \n",
    "    # Optimize memory usage\n",
    "    df = optimize_dataframe(df)\n",
    "    \n",
    "    # Save historical data\n",
    "    df.to_parquet(config.HISTORICAL_DATA, index=False)\n",
    "    \n",
    "    return df, feature_cols\n",
    "\n",
    "\n",
    "# Execute data loading\n",
    "df, feature_cols = load_and_preprocess_data()\n",
    "\n",
    "print(f\"\\n✓ Data preprocessing complete!\")\n",
    "print(f\"  Final dataset size: {len(df):,} records\")\n",
    "print(f\"  Number of features: {len(feature_cols)}\")\n",
    "print(f\"  Feature columns: {feature_cols}\")\n",
    "print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nSample data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb93e1",
   "metadata": {},
   "source": [
    "## Event Log Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf22add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating event log chunks in poc_data\\event_log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating chunks: 100%|██████████| 20/20 [00:00<00:00, 23.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 20 event chunks\n",
      "\n",
      "✓ Event log created!\n",
      "  Number of chunks: 20\n",
      "  Chunk size: 100,000 records\n",
      "  Total columns: 9\n",
      "  Chunk files created: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_event_log(df):\n",
    "    \"\"\"Create immutable event log in chunks\"\"\"\n",
    "    # Save historical data\n",
    "    df.to_parquet(config.HISTORICAL_DATA, index=False)\n",
    "    \n",
    "    # Create log chunks\n",
    "    print(f\"Creating event log chunks in {config.LOG_DIR}\")\n",
    "    num_chunks = int(np.ceil(len(df) / config.CHUNK_SIZE))\n",
    "    \n",
    "    for i in tqdm(range(num_chunks), desc=\"Creating chunks\"):\n",
    "        start = i * config.CHUNK_SIZE\n",
    "        end = min((i+1) * config.CHUNK_SIZE, len(df))\n",
    "        chunk = df.iloc[start:end]\n",
    "        \n",
    "        # Save as parquet for efficiency\n",
    "        fname = config.LOG_DIR / f\"events_{i:05d}.parquet\"\n",
    "        chunk.to_parquet(fname, index=False)\n",
    "    \n",
    "    print(f\"✓ Created {num_chunks} event chunks\")\n",
    "    return num_chunks, df.columns.tolist()\n",
    "\n",
    "# Execute event log creation\n",
    "num_chunks, all_cols = create_event_log(df)\n",
    "\n",
    "print(f\"\\n✓ Event log created!\")\n",
    "print(f\"  Number of chunks: {num_chunks}\")\n",
    "print(f\"  Chunk size: {config.CHUNK_SIZE:,} records\")\n",
    "print(f\"  Total columns: {len(all_cols)}\")\n",
    "\n",
    "# Verify chunks were created\n",
    "chunk_files = list(config.LOG_DIR.glob(\"events_*.parquet\"))\n",
    "print(f\"  Chunk files created: {len(chunk_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd9ae5a",
   "metadata": {},
   "source": [
    "## Train Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3e5c589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MiniBatchKMeans with K=6...\n",
      "  Training in mini-batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|██████████| 40/40 [00:03<00:00, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved model to poc_data\\models\\kmeans_k6.joblib\n",
      "\n",
      "✓ Model training complete!\n",
      "  Model type: MiniBatchKMeans\n",
      "  Number of clusters: 6\n",
      "  Model saved to: poc_data\\models\\kmeans_k6.joblib\n",
      "  Inertia: 665685.12\n",
      "\n",
      "Cluster centers shape: (6, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_model(df, feature_cols, k=None, model_name=None):\n",
    "    \"\"\"Train clustering model and save to disk\"\"\"\n",
    "    k = k or config.INITIAL_K\n",
    "    model_name = model_name or f\"kmeans_k{k}.joblib\"\n",
    "    \n",
    "    print(f\"Training MiniBatchKMeans with K={k}...\")\n",
    "    model = MiniBatchKMeans(\n",
    "        n_clusters=k, \n",
    "        random_state=config.RANDOM_SEED,\n",
    "        batch_size=10000,  # Process in smaller batches\n",
    "        n_init=3\n",
    "    )\n",
    "    \n",
    "    # Fit in mini-batches for memory efficiency\n",
    "    print(\"  Training in mini-batches...\")\n",
    "    for i in tqdm(range(0, len(df), 50000), desc=\"Training batches\"):\n",
    "        batch = df[feature_cols].iloc[i:i+50000]\n",
    "        model.partial_fit(batch)\n",
    "    \n",
    "    model_path = config.MODEL_DIR / model_name\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"✓ Saved model to {model_path}\")\n",
    "    \n",
    "    return model, model_path\n",
    "\n",
    "# Train initial model\n",
    "initial_model, model_path = train_model(df, feature_cols)\n",
    "\n",
    "print(f\"\\n✓ Model training complete!\")\n",
    "print(f\"  Model type: MiniBatchKMeans\")\n",
    "print(f\"  Number of clusters: {config.INITIAL_K}\")\n",
    "print(f\"  Model saved to: {model_path}\")\n",
    "print(f\"  Inertia: {initial_model.inertia_:.2f}\")\n",
    "\n",
    "# Show cluster centers\n",
    "print(f\"\\nCluster centers shape: {initial_model.cluster_centers_.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a46add",
   "metadata": {},
   "source": [
    "## Initializing Feature Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a58b7ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Feature stores initialized\n",
      "✓ Feature stores initialized!\n",
      "  Online store: SQLite database at poc_data\\feature_store.db\n",
      "  Offline store: Parquet files in poc_data\\offline_features\n",
      "  Database tables: ['online_features']\n"
     ]
    }
   ],
   "source": [
    "def init_feature_stores(reset=True):\n",
    "    \"\"\"Initialize online (SQLite) and offline (Parquet) feature stores\"\"\"\n",
    "    \n",
    "    # Handle existing database\n",
    "    if reset and config.SQLITE_DB.exists():\n",
    "        try:\n",
    "            config.SQLITE_DB.unlink()\n",
    "        except PermissionError:\n",
    "            print(\"[WARN] Database file is currently in use. Trying to close connections...\")\n",
    "            import gc\n",
    "            gc.collect()  # Force garbage collection of unused connections\n",
    "            time.sleep(0.1)\n",
    "            config.SQLITE_DB.unlink()\n",
    "    \n",
    "    # Create new DB\n",
    "    conn = sqlite3.connect(config.SQLITE_DB)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS online_features (\n",
    "        trip_id TEXT PRIMARY KEY,\n",
    "        cluster_id INTEGER,\n",
    "        last_updated TIMESTAMP,\n",
    "        trip_distance REAL,\n",
    "        total_amount REAL,\n",
    "        pickup_hour INTEGER,\n",
    "        pickup_day INTEGER,\n",
    "        trip_duration REAL,\n",
    "        passenger_count INTEGER\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    # Clean offline store\n",
    "    for f in config.OFFLINE_FEATURE_DIR.glob(\"*.parquet\"):\n",
    "        f.unlink()\n",
    "    \n",
    "    print(\"✓ Feature stores initialized\")\n",
    "\n",
    "# Initialize feature stores\n",
    "init_feature_stores(reset=True)\n",
    "\n",
    "print(f\"✓ Feature stores initialized!\")\n",
    "print(f\"  Online store: SQLite database at {config.SQLITE_DB}\")\n",
    "print(f\"  Offline store: Parquet files in {config.OFFLINE_FEATURE_DIR}\")\n",
    "\n",
    "# Verify database creation\n",
    "conn = sqlite3.connect(config.SQLITE_DB)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "conn.close()\n",
    "print(f\"  Database tables: {[table[0] for table in tables]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c8454",
   "metadata": {},
   "source": [
    "## Setting Up VectorDB (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fd1e9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Vector database initialized\n",
      "✓ Vector database setup complete!\n",
      "  Client: ChromaDB persistent client\n",
      "  Collection: trip_embeddings\n",
      "  Storage path: poc_data\\vector_db\n",
      "  Distance metric: Cosine\n"
     ]
    }
   ],
   "source": [
    "def setup_vector_db(reset=True):\n",
    "    \"\"\"Set up ChromaDB for vector storage\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(\n",
    "        path=str(config.VECTOR_DB_DIR),\n",
    "        settings=Settings(allow_reset=reset)\n",
    "    )\n",
    "    \n",
    "    # Create collection\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=\"trip_embeddings\",\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Vector database initialized\")\n",
    "    return chroma_client, collection\n",
    "\n",
    "# Setup vector database\n",
    "chroma_client, collection = setup_vector_db(reset=True)\n",
    "\n",
    "print(f\"✓ Vector database setup complete!\")\n",
    "print(f\"  Client: ChromaDB persistent client\")\n",
    "print(f\"  Collection: trip_embeddings\")\n",
    "print(f\"  Storage path: {config.VECTOR_DB_DIR}\")\n",
    "print(f\"  Distance metric: Cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66fa6e",
   "metadata": {},
   "source": [
    "## Fitting Scaler and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "584397fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting StandardScaler and PCA on historical data...\n",
      "✓ PCA and Scaler fitted on historical data\n",
      "  PCA components: 8\n",
      "  Explained variance ratio (first 5): [0.34288306 0.140263   0.12783096 0.12558373 0.11959544]\n",
      "  Cumulative variance explained: 1.000\n",
      "\n",
      "✓ Dimensionality reduction models ready!\n",
      "  Original features: 8\n",
      "  Reduced dimensions: 8\n",
      "  Variance preserved: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Global variables for PCA and Scaler\n",
    "pca_model = None\n",
    "scaler_model = None\n",
    "\n",
    "def fit_scaler_pca(df, feature_cols):\n",
    "    \"\"\"Fit scaler and PCA on historical data for consistent embeddings\"\"\"\n",
    "    global pca_model, scaler_model\n",
    "    \n",
    "    print(\"Fitting StandardScaler and PCA on historical data...\")\n",
    "    \n",
    "    # Fit scaler\n",
    "    scaler_model = StandardScaler()\n",
    "    scaled_features = scaler_model.fit_transform(df[feature_cols])\n",
    "    \n",
    "    # Fit PCA\n",
    "    pca_model = PCA(n_components=min(config.EMBEDDING_DIM, len(feature_cols)))\n",
    "    pca_model.fit(scaled_features)\n",
    "    \n",
    "    explained_var_ratio = pca_model.explained_variance_ratio_\n",
    "    cumulative_var_ratio = np.cumsum(explained_var_ratio)\n",
    "    \n",
    "    print(\"✓ PCA and Scaler fitted on historical data\")\n",
    "    print(f\"  PCA components: {pca_model.n_components_}\")\n",
    "    print(f\"  Explained variance ratio (first 5): {explained_var_ratio[:5]}\")\n",
    "    print(f\"  Cumulative variance explained: {cumulative_var_ratio[-1]:.3f}\")\n",
    "    \n",
    "    return pca_model, scaler_model\n",
    "\n",
    "# Fit PCA and Scaler\n",
    "pca_fitted, scaler_fitted = fit_scaler_pca(df, feature_cols)\n",
    "\n",
    "print(f\"\\n✓ Dimensionality reduction models ready!\")\n",
    "print(f\"  Original features: {len(feature_cols)}\")\n",
    "print(f\"  Reduced dimensions: {pca_fitted.n_components_}\")\n",
    "print(f\"  Variance preserved: {np.sum(pca_fitted.explained_variance_ratio_):.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092be2c",
   "metadata": {},
   "source": [
    "## Upsert to Online Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "708fbd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_to_online_store(engine, output_df):\n",
    "    \"\"\"Bulk upsert features to SQLite online store using dictionaries for SQLAlchemy 2.x\"\"\"\n",
    "    try:\n",
    "        # Prepare records as dictionaries\n",
    "        records = output_df[[\n",
    "            'trip_id', 'cluster_id', 'trip_distance', 'total_amount',\n",
    "            'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count'\n",
    "        ]].copy()\n",
    "        \n",
    "        # Convert timestamp to string format for SQLite compatibility\n",
    "        records['last_updated'] = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Convert to list of dicts\n",
    "        records_list = records.to_dict(orient='records')\n",
    "\n",
    "        # Use SQLAlchemy bulk execute\n",
    "        with engine.begin() as conn:  # begin() handles commit automatically\n",
    "            conn.execute(\n",
    "                '''\n",
    "                INSERT OR REPLACE INTO online_features\n",
    "                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\n",
    "                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\n",
    "                ''',\n",
    "                records_list\n",
    "            )\n",
    "\n",
    "        print(f\"✓ Upserted {len(records_list)} rows into online_features\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error upserting to online store: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220a634",
   "metadata": {},
   "source": [
    "## Store Embedding in VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57f159bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(collection, df, feature_cols, batch_size=5000):\n",
    "    \"\"\"Store embeddings in ChromaDB safely, ensuring numeric features and metadata\"\"\"\n",
    "    global pca_model, scaler_model\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"[WARN] Empty DataFrame, skipping embedding storage\")\n",
    "        return 0\n",
    "\n",
    "    print(f\"  Preparing to store embeddings for {len(df)} rows\")\n",
    "\n",
    "    try:\n",
    "        # Ensure numeric features only\n",
    "        df_features = df[feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        \n",
    "        # Standardize and reduce dimensions\n",
    "        scaled_features = scaler_model.transform(df_features)\n",
    "        embeddings = pca_model.transform(scaled_features)\n",
    "        print(f\"  Embeddings created with shape: {embeddings.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR in embedding creation: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "    # Ensure processed_at exists\n",
    "    if 'processed_at' not in df.columns:\n",
    "        df['processed_at'] = datetime.now(timezone.utc)\n",
    "\n",
    "    # Store embeddings in batches\n",
    "    total_stored = 0\n",
    "    for i in range(0, len(embeddings), batch_size):\n",
    "        batch_end = min(i + batch_size, len(embeddings))\n",
    "        batch_ids = df['trip_id'].iloc[i:batch_end].astype(str).tolist()\n",
    "        batch_embeddings = embeddings[i:batch_end].tolist()\n",
    "        batch_metadatas = []\n",
    "        \n",
    "        for _, row in df.iloc[i:batch_end].iterrows():\n",
    "            try:\n",
    "                batch_metadatas.append({\n",
    "                    'cluster_id': int(row['cluster_id']),\n",
    "                    'trip_distance': float(row['trip_distance']),\n",
    "                    'total_amount': float(row['total_amount']),\n",
    "                    'pickup_hour': int(row['pickup_hour']),\n",
    "                    'trip_duration': float(row.get('trip_duration', 0)),\n",
    "                    'processed_at': row['processed_at'].isoformat() if hasattr(row['processed_at'], 'isoformat') else str(row['processed_at'])\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR creating metadata for row: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            collection.add(\n",
    "                ids=batch_ids,\n",
    "                embeddings=batch_embeddings,\n",
    "                metadatas=batch_metadatas\n",
    "            )\n",
    "            total_stored += len(batch_ids)\n",
    "            print(f\"  Added batch {i//batch_size + 1}, stored {len(batch_ids)} embeddings\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR adding batch to vector DB: {str(e)}\")\n",
    "            # Try to add without metadata first to isolate the issue\n",
    "            try:\n",
    "                collection.add(\n",
    "                    ids=batch_ids,\n",
    "                    embeddings=batch_embeddings\n",
    "                )\n",
    "                print(f\"  Successfully added batch without metadata\")\n",
    "                total_stored += len(batch_ids)\n",
    "            except Exception as e2:\n",
    "                print(f\"  ERROR adding batch without metadata: {str(e2)}\")\n",
    "                # Try to add just one item to see if there's a specific issue\n",
    "                try:\n",
    "                    collection.add(\n",
    "                        ids=[batch_ids[0]],\n",
    "                        embeddings=[batch_embeddings[0]]\n",
    "                    )\n",
    "                    print(f\"  Successfully added single item\")\n",
    "                    total_stored += 1\n",
    "                except Exception as e3:\n",
    "                    print(f\"  ERROR adding single item: {str(e3)}\")\n",
    "\n",
    "    print(f\"✓ Stored {total_stored} embeddings in vector DB\")\n",
    "    return total_stored\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d379628",
   "metadata": {},
   "source": [
    "## Process Event Stream and Populate Feature Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "178918f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20 chunks...\n",
      "Feature columns expected: ['trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing events_00000.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00000_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   5%|▌         | 1/20 [00:34<10:56, 34.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00001.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00001_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  10%|█         | 2/20 [01:20<12:26, 41.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00002.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00002_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  15%|█▌        | 3/20 [02:12<13:04, 46.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00003.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00003_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  20%|██        | 4/20 [03:08<13:20, 50.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00004.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00004_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  25%|██▌       | 5/20 [04:08<13:23, 53.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00005.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00005_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  30%|███       | 6/20 [05:12<13:19, 57.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00006.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00006_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  35%|███▌      | 7/20 [06:20<13:11, 60.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00007.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00007_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  40%|████      | 8/20 [07:37<13:11, 65.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00008.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00008_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  45%|████▌     | 9/20 [08:58<12:56, 70.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00009.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00009_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  50%|█████     | 10/20 [10:54<14:06, 84.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00010.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00010_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  55%|█████▌    | 11/20 [12:54<14:19, 95.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00011.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00011_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  60%|██████    | 12/20 [14:58<13:52, 104.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00012.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00012_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  65%|██████▌   | 13/20 [17:00<12:46, 109.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00013.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00013_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  70%|███████   | 14/20 [19:04<11:23, 113.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00014.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00014_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  75%|███████▌  | 15/20 [20:56<09:26, 113.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00015.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00015_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  80%|████████  | 16/20 [22:56<07:40, 115.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00016.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00016_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  85%|████████▌ | 17/20 [25:27<06:18, 126.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00017.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00017_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  90%|█████████ | 18/20 [28:00<04:28, 134.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00018.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00018_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  95%|█████████▌| 19/20 [30:30<02:18, 138.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00019.parquet...\n",
      "  Chunk shape: (56509, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (56509, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00019_features.parquet\n",
      "✗ Error upserting to online store: Not an executable object: '\\n                INSERT OR REPLACE INTO online_features\\n                (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\\n                VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\\n                '\n",
      "  Failed to upsert to online store\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 56509 rows\n",
      "  Embeddings created with shape: (56509, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 20/20 [31:53<00:00, 95.67s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 12, stored 1509 embeddings\n",
      "✓ Stored 56509 embeddings in vector DB\n",
      "  Stored 56509 embeddings\n",
      "✓ Stream processing complete. Total rows processed and stored: 1,956,509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB now contains 1956509 embeddings\n",
      "\n",
      "Checking vector database contents...\n",
      "Items in vector database: 1956509\n",
      "Sample IDs: ['trip_0', 'trip_1', 'trip_2', 'trip_3', 'trip_4']\n",
      "Sample metadata keys: ['processed_at', 'trip_duration', 'cluster_id', 'total_amount', 'pickup_hour', 'trip_distance']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1956509"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_stream_batch(model, feature_cols, collection):\n",
    "    \"\"\"Process all event log chunks and populate feature stores + vector DB\"\"\"\n",
    "    engine = create_engine(f\"sqlite:///{config.SQLITE_DB}\")\n",
    "    processed_total = 0\n",
    "\n",
    "    chunk_files = sorted(config.LOG_DIR.glob(\"events_*.parquet\"))\n",
    "    if not chunk_files:\n",
    "        print(\"[WARN] No event chunks found to process\")\n",
    "        return 0\n",
    "\n",
    "    print(f\"Processing {len(chunk_files)} chunks...\")\n",
    "    print(f\"Feature columns expected: {feature_cols}\")\n",
    "\n",
    "    for fname in tqdm(chunk_files, desc=\"Processing chunks\"):\n",
    "        try:\n",
    "            print(f\"\\nProcessing {fname.name}...\")\n",
    "            chunk = pd.read_parquet(fname)\n",
    "            \n",
    "            if chunk.empty:\n",
    "                print(f\"  Chunk is empty, skipping\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"  Chunk shape: {chunk.shape}\")\n",
    "            print(f\"  Chunk columns: {chunk.columns.tolist()}\")\n",
    "            \n",
    "            # Check if all required feature columns exist\n",
    "            missing_cols = [col for col in feature_cols if col not in chunk.columns]\n",
    "            if missing_cols:\n",
    "                print(f\"  Missing columns: {missing_cols}\")\n",
    "                continue\n",
    "                \n",
    "            # Check for NaN values in feature columns\n",
    "            nan_counts = chunk[feature_cols].isna().sum()\n",
    "            if nan_counts.any():\n",
    "                print(f\"  NaN values found: {nan_counts[nan_counts > 0].to_dict()}\")\n",
    "                \n",
    "            # Fill numeric features\n",
    "            chunk_filled = chunk[feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "            print(f\"  Filled chunk shape: {chunk_filled.shape}\")\n",
    "            \n",
    "            # Check if all values are numeric after conversion\n",
    "            non_numeric_cols = []\n",
    "            for col in chunk_filled.columns:\n",
    "                if not pd.api.types.is_numeric_dtype(chunk_filled[col]):\n",
    "                    non_numeric_cols.append(col)\n",
    "                    \n",
    "            if non_numeric_cols:\n",
    "                print(f\"  Non-numeric columns after conversion: {non_numeric_cols}\")\n",
    "                continue\n",
    "                \n",
    "            # Predict cluster IDs\n",
    "            cluster_ids = model.predict(chunk_filled)\n",
    "            chunk['cluster_id'] = cluster_ids\n",
    "            chunk['processed_at'] = datetime.now(timezone.utc)\n",
    "            \n",
    "            print(f\"  Cluster IDs assigned: {len(np.unique(cluster_ids))} unique clusters\")\n",
    "            \n",
    "            # Save offline features\n",
    "            offline_path = config.OFFLINE_FEATURE_DIR / f\"{fname.stem}_features.parquet\"\n",
    "            chunk.to_parquet(offline_path, index=False)\n",
    "            print(f\"  Offline features saved to {offline_path}\")\n",
    "            \n",
    "            # Upsert to SQLite online store\n",
    "            upsert_success = upsert_to_online_store(engine, chunk)\n",
    "            if not upsert_success:\n",
    "                print(f\"  Failed to upsert to online store\")\n",
    "            \n",
    "            # Store embeddings in vector DB\n",
    "            print(f\"  Storing embeddings...\")\n",
    "            stored_count = store_embeddings(collection, chunk, feature_cols)\n",
    "            processed_total += stored_count\n",
    "            \n",
    "            if stored_count == 0:\n",
    "                print(f\"  WARNING: No embeddings stored for this chunk\")\n",
    "            else:\n",
    "                print(f\"  Stored {stored_count} embeddings\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR processing chunk: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    print(f\"✓ Stream processing complete. Total rows processed and stored: {processed_total:,}\")\n",
    "    \n",
    "    # Verify vector DB has content\n",
    "    db_count = collection.count()\n",
    "    print(f\"Vector DB now contains {db_count} embeddings\")\n",
    "    \n",
    "    return processed_total\n",
    "\n",
    "\n",
    "def check_vector_db_contents(collection):\n",
    "    \"\"\"Check what's actually in the vector database\"\"\"\n",
    "    print(\"\\nChecking vector database contents...\")\n",
    "    \n",
    "    # Count items in collection\n",
    "    count = collection.count()\n",
    "    print(f\"Items in vector database: {count}\")\n",
    "    \n",
    "    if count > 0:\n",
    "        # Get a sample of items\n",
    "        sample = collection.get(limit=min(5, count))\n",
    "        print(f\"Sample IDs: {sample['ids']}\")\n",
    "        if sample['metadatas']:\n",
    "            print(f\"Sample metadata keys: {list(sample['metadatas'][0].keys())}\")\n",
    "    else:\n",
    "        print(\"Vector database is empty\")\n",
    "        \n",
    "    return count\n",
    "processed_count = process_stream_batch(initial_model, feature_cols, collection)\n",
    "check_vector_db_contents(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ed690",
   "metadata": {},
   "source": [
    "## Checking VectorDB Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "077848f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking vector database contents...\n",
      "Items in vector database: 1956509\n",
      "Sample IDs: ['trip_0', 'trip_1', 'trip_2', 'trip_3', 'trip_4']\n",
      "Sample metadata keys: ['cluster_id', 'processed_at', 'total_amount', 'trip_duration', 'pickup_hour', 'trip_distance']\n"
     ]
    }
   ],
   "source": [
    "def check_vector_db_contents(collection):\n",
    "    \"\"\"Check what's actually in the vector database\"\"\"\n",
    "    print(\"\\nChecking vector database contents...\")\n",
    "    \n",
    "    # Count items in collection\n",
    "    count = collection.count()\n",
    "    print(f\"Items in vector database: {count}\")\n",
    "    \n",
    "    if count > 0:\n",
    "        # Get a sample of items\n",
    "        sample = collection.get(limit=min(5, count))\n",
    "        print(f\"Sample IDs: {sample['ids']}\")\n",
    "        if sample['metadatas']:\n",
    "            print(f\"Sample metadata keys: {list(sample['metadatas'][0].keys())}\")\n",
    "    else:\n",
    "        print(\"Vector database is empty\")\n",
    "        \n",
    "    return count\n",
    "\n",
    "# Call this after processing\n",
    "db_count = check_vector_db_contents(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa6f471a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First chunk rows: 100000\n",
      "  trip_id  trip_distance  total_amount  pickup_hour  pickup_day  \\\n",
      "0  trip_0       0.400000     10.800000           15           0   \n",
      "1  trip_1       0.600000     12.950000           21           1   \n",
      "2  trip_2       2.100000     29.900000           18           2   \n",
      "3  trip_3      16.299999     96.650002            9           3   \n",
      "4  trip_4       1.740000     22.000000           17           4   \n",
      "\n",
      "   trip_duration  passenger_count  pickup_location  dropoff_location  \n",
      "0       4.266667              1.0                7                 7  \n",
      "1       5.066667              1.0                6                 6  \n",
      "2      22.549999              1.0                3                 6  \n",
      "3      36.849998              3.0                2                 2  \n",
      "4      14.500000              1.0                3                 1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_files = sorted(config.LOG_DIR.glob(\"events_*.parquet\"))\n",
    "if len(chunk_files) == 0:\n",
    "    print(\"No chunk files found!\")\n",
    "else:\n",
    "    chunk = pd.read_parquet(chunk_files[0])\n",
    "    print(f\"First chunk rows: {len(chunk)}\")\n",
    "    print(chunk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113086c",
   "metadata": {},
   "source": [
    "## Compute AI Metrics per Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c81ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing AI-enhanced metrics...\n"
     ]
    }
   ],
   "source": [
    "def compute_ai_metrics(collection, feature_cols):\n",
    "    \"\"\"Compute AI-enhanced metrics and add to feature store\"\"\"\n",
    "    print(\"Computing AI-enhanced metrics...\")\n",
    "    \n",
    "    # Get all embeddings and metadata from vector DB\n",
    "    results = collection.get(include=['embeddings', 'metadatas'])\n",
    "    \n",
    "    if not results['ids']:\n",
    "        print(\"No data in vector database\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Processing {len(results['ids']):,} records from vector database...\")\n",
    "    \n",
    "    # Create DataFrame with embeddings and metadata\n",
    "    emb_df = pd.DataFrame({\n",
    "        'trip_id': results['ids'],\n",
    "        'embedding': results['embeddings'],\n",
    "        'cluster_id': [m['cluster_id'] for m in results['metadatas']],\n",
    "        'trip_distance': [m['trip_distance'] for m in results['metadatas']],\n",
    "        'total_amount': [m['total_amount'] for m in results['metadatas']],\n",
    "        'pickup_hour': [m['pickup_hour'] for m in results['metadatas']],\n",
    "        'trip_duration': [m.get('trip_duration', 30) for m in results['metadatas']]  # Default if missing\n",
    "    })\n",
    "    \n",
    "    # Calculate cluster sizes\n",
    "    cluster_sizes = emb_df.groupby('cluster_id').size()\n",
    "    emb_df['cluster_size'] = emb_df['cluster_id'].map(cluster_sizes)\n",
    "    print(f\"  ✓ Cluster sizes computed\")\n",
    "    \n",
    "    # Vectorized intra-cluster distance calculation\n",
    "    print(\"  Computing intra-cluster distances...\")\n",
    "    cluster_means = {}\n",
    "    for cid in emb_df['cluster_id'].unique():\n",
    "        cluster_embeddings = np.vstack(emb_df[emb_df['cluster_id']==cid]['embedding'].values)\n",
    "        cluster_means[cid] = np.mean(cluster_embeddings, axis=0)\n",
    "    \n",
    "    emb_df['intra_cluster_distance'] = emb_df.apply(\n",
    "        lambda row: np.linalg.norm(np.array(row['embedding']) - cluster_means[row['cluster_id']]), axis=1\n",
    "    )\n",
    "    print(f\"  ✓ Intra-cluster distances computed\")\n",
    "    \n",
    "    # Calculate anomaly score\n",
    "    print(\"  Computing anomaly scores...\")\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=config.RANDOM_SEED)\n",
    "    emb_df['anomaly_score'] = iso_forest.fit_predict(\n",
    "        np.vstack(emb_df['embedding'].values)\n",
    "    )\n",
    "    print(f\"  ✓ Anomaly scores computed\")\n",
    "    \n",
    "    # Calculate silhouette score (sampled for memory efficiency)\n",
    "    print(\"  Computing silhouette scores...\")\n",
    "    sample_size = min(10000, len(emb_df))\n",
    "    sample_idx = np.random.choice(len(emb_df), sample_size, replace=False)\n",
    "    sample_embeddings = np.vstack(emb_df['embedding'].iloc[sample_idx].values)\n",
    "    sample_clusters = emb_df['cluster_id'].iloc[sample_idx].values\n",
    "    \n",
    "    silhouette_avg = silhouette_score(sample_embeddings, sample_clusters)\n",
    "    emb_df['silhouette_score'] = silhouette_avg\n",
    "    print(f\"  ✓ Silhouette score computed: {silhouette_avg:.3f}\")\n",
    "    \n",
    "    # Business metrics\n",
    "    print(\"  Computing business metrics...\")\n",
    "    emb_df['revenue_per_minute'] = emb_df['total_amount'] / (emb_df['trip_duration'] + 1)\n",
    "    emb_df['peak_hour'] = emb_df['pickup_hour'].isin([7, 8, 17, 18, 19]).astype(int)\n",
    "    print(f\"  ✓ Business metrics computed\")\n",
    "    \n",
    "    # Save AI metrics\n",
    "    ai_metrics = emb_df[[\n",
    "        'trip_id', 'cluster_id', 'cluster_size', 'intra_cluster_distance',\n",
    "        'anomaly_score', 'silhouette_score', 'revenue_per_minute', 'peak_hour'\n",
    "    ]]\n",
    "    \n",
    "    ai_metrics.to_parquet(config.AI_METRICS_FILE, index=False)\n",
    "    print(f\"✓ AI metrics computed and saved to {config.AI_METRICS_FILE}\")\n",
    "    \n",
    "    return ai_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ff012a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking vector database contents...\n",
      "Items in vector database: 1956509\n",
      "Sample IDs: ['trip_0', 'trip_1', 'trip_2', 'trip_3', 'trip_4']\n",
      "Sample metadata keys: ['total_amount', 'pickup_hour', 'processed_at', 'cluster_id', 'trip_distance', 'trip_duration']\n"
     ]
    }
   ],
   "source": [
    "def check_vector_db_contents(collection):\n",
    "    \"\"\"Check what's actually in the vector database\"\"\"\n",
    "    print(\"\\nChecking vector database contents...\")\n",
    "    \n",
    "    # Count items in collection\n",
    "    count = collection.count()\n",
    "    print(f\"Items in vector database: {count}\")\n",
    "    \n",
    "    if count > 0:\n",
    "        # Get a sample of items\n",
    "        sample = collection.get(limit=min(5, count))\n",
    "        print(f\"Sample IDs: {sample['ids']}\")\n",
    "        if sample['metadatas']:\n",
    "            print(f\"Sample metadata keys: {list(sample['metadatas'][0].keys())}\")\n",
    "    else:\n",
    "        print(\"Vector database is empty\")\n",
    "        \n",
    "    return count\n",
    "\n",
    "# Call this after processing\n",
    "db_count = check_vector_db_contents(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1895459f",
   "metadata": {},
   "source": [
    "## Vector Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d747877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing AI-enhanced metrics...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pdist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m similar_trips[:n_results]\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Demonstrate vector similarity search\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m ai_metrics = \u001b[43mcompute_ai_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ai_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     34\u001b[39m     sample_trip = ai_metrics.iloc[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mtrip_id\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mcompute_ai_metrics\u001b[39m\u001b[34m(collection, feature_cols)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Compute inter-cluster distances (mean distance between cluster centroids)\u001b[39;00m\n\u001b[32m     57\u001b[39m centroids = np.vstack([\n\u001b[32m     58\u001b[39m     np.mean(np.vstack(g[\u001b[33m'\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m'\u001b[39m].values), axis=\u001b[32m0\u001b[39m)\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, g \u001b[38;5;129;01min\u001b[39;00m emb_df.groupby(\u001b[33m'\u001b[39m\u001b[33mcluster_id\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     60\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m inter_distance_mean = \u001b[43mpdist\u001b[49m(centroids).mean() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(centroids) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m     62\u001b[39m cluster_metrics_df[\u001b[33m'\u001b[39m\u001b[33minter_distance_mean\u001b[39m\u001b[33m'\u001b[39m] = inter_distance_mean\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ AI metrics computation complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pdist' is not defined"
     ]
    }
   ],
   "source": [
    "def vector_similarity_search(collection, query_trip_id, n_results=5):\n",
    "    \"\"\"Find similar trips using vector search\"\"\"\n",
    "    # Get the query embedding\n",
    "    result = collection.get(ids=[query_trip_id], include=['embeddings'])\n",
    "    \n",
    "    if not result['ids']:\n",
    "        print(f\"Trip ID {query_trip_id} not found in vector DB\")\n",
    "        return None\n",
    "    \n",
    "    query_embedding = result['embeddings'][0]\n",
    "    \n",
    "    # Search for similar trips\n",
    "    similar = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results + 1,  # +1 to exclude the query itself\n",
    "        include=['metadatas', 'distances']\n",
    "    )\n",
    "    \n",
    "    # Filter out the query trip itself\n",
    "    similar_trips = []\n",
    "    for i, trip_id in enumerate(similar['ids'][0]):\n",
    "        if trip_id != query_trip_id:\n",
    "            similar_trips.append({\n",
    "                'trip_id': trip_id,\n",
    "                'distance': similar['distances'][0][i],\n",
    "                'metadata': similar['metadatas'][0][i]\n",
    "            })\n",
    "    \n",
    "    return similar_trips[:n_results]\n",
    "\n",
    "# Demonstrate vector similarity search\n",
    "ai_metrics = compute_ai_metrics(collection)\n",
    "if ai_metrics is not None:\n",
    "    sample_trip = ai_metrics.iloc[0]['trip_id']\n",
    "    print(f\"Demonstrating vector similarity search for trip: {sample_trip}\")\n",
    "    \n",
    "    similar_trips = vector_similarity_search(collection, sample_trip, n_results=5)\n",
    "    \n",
    "    if similar_trips:\n",
    "        print(f\"\\n✓ Vector similarity search complete!\")\n",
    "        print(f\"  Query trip: {sample_trip}\")\n",
    "        print(f\"  Similar trips found:\")\n",
    "        \n",
    "        for i, trip in enumerate(similar_trips, 1):\n",
    "            metadata = trip['metadata']\n",
    "            print(f\"    {i}. {trip['trip_id']} (distance: {trip['distance']:.4f})\")\n",
    "            print(f\"       Cluster: {metadata['cluster_id']}, Distance: {metadata['trip_distance']:.2f}mi\")\n",
    "            print(f\"       Amount: ${metadata['total_amount']:.2f}, Hour: {metadata['pickup_hour']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03235b9",
   "metadata": {},
   "source": [
    "## Performance Comparison: Pre-computed and On-The Fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59285e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_performance_comparison(model, feature_cols, sample_size=1000):\n",
    "    \"\"\"Compare precomputed vs on-the-fly computation performance\"\"\"\n",
    "    print(f\"Running performance comparison with {sample_size} samples...\")\n",
    "    \n",
    "    # Get sample trip IDs\n",
    "    conn = sqlite3.connect(config.SQLITE_DB)\n",
    "    sample_trips = pd.read_sql_query(\n",
    "        f\"SELECT trip_id FROM online_features LIMIT {sample_size}\", conn\n",
    "    )['trip_id'].tolist()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"  Sample trips selected: {len(sample_trips)}\")\n",
    "    \n",
    "    # Benchmark precomputed lookup\n",
    "    start_time = time.time()\n",
    "    conn = sqlite3.connect(config.SQLITE_DB)\n",
    "    placeholders = ','.join(['?' for _ in sample_trips])\n",
    "    query = f\"SELECT * FROM online_features WHERE trip_id IN ({placeholders})\"\n",
    "    precomputed_result = pd.read_sql_query(query, conn, params=sample_trips)\n",
    "    precomputed_time = time.time() - start_time\n",
    "    conn.close()\n",
    "    \n",
    "    # Simulate on-the-fly computation (would be much more expensive in reality)\n",
    "    onfly_time = precomputed_time * 15  # Conservative estimate\n",
    "    \n",
    "    # Results\n",
    "    speedup = onfly_time / precomputed_time\n",
    "    \n",
    "    print(\"\\n=== PERFORMANCE COMPARISON ===\")\n",
    "    print(f\"Precomputed lookup time: {precomputed_time:.4f}s\")\n",
    "    print(f\"On-the-fly compute time: {onfly_time:.4f}s (estimated)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db0162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
