{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0286b616",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2581670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sqlite3\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from tqdm import tqdm\n",
    "from sqlalchemy import create_engine, text\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import silhouette_score\n",
    "import urllib.request\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import chromadb\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283abb78",
   "metadata": {},
   "source": [
    "## Configuring Directories and Processing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31b3abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration initialized\n",
      "  - Data directory: poc_data\n",
      "  - Chunk size: 100,000\n",
      "  - Total samples: 2,000,000\n",
      "  - Initial K clusters: 6\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    # Directories\n",
    "    DATA_DIR = Path(\"./poc_data\")\n",
    "    LOG_DIR = DATA_DIR / \"event_log\"\n",
    "    OFFLINE_FEATURE_DIR = DATA_DIR / \"offline_features\"\n",
    "    MODEL_DIR = DATA_DIR / \"models\"\n",
    "    VECTOR_DB_DIR = DATA_DIR / \"vector_db\"\n",
    "    \n",
    "    # Files\n",
    "    SQLITE_DB = DATA_DIR / \"feature_store.db\"\n",
    "    HISTORICAL_DATA = DATA_DIR / \"historical_full.parquet\"\n",
    "    AI_METRICS_FILE = DATA_DIR / \"ai_metrics.parquet\"\n",
    "    \n",
    "    # Processing parameters\n",
    "    CHUNK_SIZE = 100000\n",
    "    TOTAL_SAMPLES = 2000000  # Reduced for 16GB RAM\n",
    "    RANDOM_SEED = 42\n",
    "    INITIAL_K = 6\n",
    "    REPROCESS_K = 8\n",
    "    EMBEDDING_DIM = 128  # Reduced dimensionality for memory efficiency\n",
    "    \n",
    "    # NYC Taxi Data URL\n",
    "    TAXI_DATA_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "print(\"Configuration initialized\")\n",
    "print(f\"  - Data directory: {config.DATA_DIR}\")\n",
    "print(f\"  - Chunk size: {config.CHUNK_SIZE:,}\")\n",
    "print(f\"  - Total samples: {config.TOTAL_SAMPLES:,}\")\n",
    "print(f\"  - Initial K clusters: {config.INITIAL_K}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b823fe30",
   "metadata": {},
   "source": [
    "## Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22470434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All directories created\n",
      "\n",
      "Directory structure created:\n",
      "poc_data\n",
      "poc_data\\event_log\n",
      "poc_data\\offline_features\n",
      "poc_data\\models\n",
      "poc_data\\vector_db\n"
     ]
    }
   ],
   "source": [
    "def setup_directories():\n",
    "    directories = [\n",
    "        config.DATA_DIR, config.LOG_DIR, config.OFFLINE_FEATURE_DIR,\n",
    "        config.MODEL_DIR, config.VECTOR_DB_DIR\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"✓ All directories created\")\n",
    "\n",
    "\n",
    "setup_directories()\n",
    "\n",
    "# Display directory structure\n",
    "print(\"\\nDirectory structure created:\")\n",
    "for directory in [config.DATA_DIR, config.LOG_DIR, config.OFFLINE_FEATURE_DIR, config.MODEL_DIR, config.VECTOR_DB_DIR]:\n",
    "    print(f\"{directory}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c946b",
   "metadata": {},
   "source": [
    "## Feature Engineering & Loading Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc4db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NYC taxi data...\n",
      "  ✓ Loading cached data...\n",
      "  Original dataset size: 3,066,766 records\n",
      "  Dataset size after dropping duplicates: 3,066,766 records\n",
      "  Dataset size after dropping NaNs: 2,995,023 records\n",
      "  Sampled to: 2,000,000 records\n",
      "\n",
      "✓ Data preprocessing complete!\n",
      "  Final dataset size: 1,956,509 records\n",
      "  Number of features: 8\n",
      "  Feature columns: ['trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Memory usage: 176.20 MB\n",
      "\n",
      "Sample data:\n",
      "        trip_id  trip_distance  total_amount  pickup_hour  pickup_day  \\\n",
      "2858416  trip_0       0.400000     10.800000           15           0   \n",
      "873744   trip_1       0.600000     12.950000           21           1   \n",
      "2361819  trip_2       2.100000     29.900000           18           2   \n",
      "1703896  trip_3      16.299999     96.650002            9           3   \n",
      "475582   trip_4       1.740000     22.000000           17           4   \n",
      "\n",
      "         trip_duration  passenger_count  pickup_location  dropoff_location  \n",
      "2858416       4.266667              1.0                7                 7  \n",
      "873744        5.066667              1.0                6                 6  \n",
      "2361819      22.549999              1.0                3                 6  \n",
      "1703896      36.849998              3.0                2                 2  \n",
      "475582       14.500000              1.0                3                 1  \n"
     ]
    }
   ],
   "source": [
    "def create_taxi_features(df):\n",
    "    # Filter reasonable values\n",
    "    df = df[\n",
    "        (df.trip_distance > 0) & \n",
    "        (df.trip_distance < 100) &\n",
    "        (df.total_amount > 0) &\n",
    "        (df.total_amount < 200)\n",
    "    ].copy()\n",
    "    \n",
    "    # Creating time-based features\n",
    "    df['pickup_hour'] = df.tpep_pickup_datetime.dt.hour\n",
    "    df['pickup_day'] = df.tpep_pickup_datetime.dt.dayofweek\n",
    "    df['trip_duration'] = (df.tpep_dropoff_datetime - df.tpep_pickup_datetime).dt.total_seconds() / 60\n",
    "    \n",
    "    # Filter out unrealistic durations\n",
    "    df = df[(df.trip_duration > 0) & (df.trip_duration < 180)]\n",
    "    \n",
    "    # Create geospatial features\n",
    "    df['pickup_location'] = (df['PULocationID'] % 10).astype('category').cat.codes\n",
    "    df['dropoff_location'] = (df['DOLocationID'] % 10).astype('category').cat.codes\n",
    "    \n",
    "    # Select features for clustering\n",
    "    feature_cols = [\n",
    "        'trip_distance', 'total_amount', 'pickup_hour', \n",
    "        'pickup_day', 'trip_duration', 'passenger_count',\n",
    "        'pickup_location', 'dropoff_location'\n",
    "    ]\n",
    "    \n",
    "    # Add unique ID\n",
    "    df['trip_id'] = [f\"trip_{i}\" for i in range(len(df))]\n",
    "    \n",
    "    return df[['trip_id'] + feature_cols], feature_cols\n",
    "\n",
    "def optimize_dataframe(df):\n",
    "    # Downcast numeric types\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    # Convert objects to category where possible\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if df[col].nunique() / len(df) < 0.5:  # If cardinality < 50%\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    print(\"Loading NYC taxi data...\")\n",
    "    # Download data if not exists\n",
    "    local_data_path = config.DATA_DIR / \"nyc_taxi_data.parquet\"\n",
    "    \n",
    "    if not local_data_path.exists():\n",
    "        print(\"  Downloading data from NYC OpenData...\")\n",
    "        df = pd.read_parquet(config.TAXI_DATA_URL)\n",
    "        df.to_parquet(local_data_path)\n",
    "        print(\"  ✓ Data downloaded and cached locally\")\n",
    "    else:\n",
    "        print(\"  ✓ Loading cached data...\")\n",
    "        df = pd.read_parquet(local_data_path)\n",
    "    \n",
    "    print(f\"  Original dataset size: {len(df):,} records\")\n",
    "    \n",
    "    # Drop duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"  Dataset size after dropping duplicates: {len(df):,} records\")\n",
    "    \n",
    "    # Drop rows with NaNs\n",
    "    df = df.dropna()\n",
    "    print(f\"  Dataset size after dropping NaNs: {len(df):,} records\")\n",
    "    \n",
    "    # Sample to fit memory constraints\n",
    "    if len(df) > config.TOTAL_SAMPLES:\n",
    "        df = df.sample(config.TOTAL_SAMPLES, random_state=config.RANDOM_SEED)\n",
    "        print(f\"  Sampled to: {len(df):,} records\")\n",
    "    \n",
    "    # Create features for clustering\n",
    "    df, feature_cols = create_taxi_features(df)\n",
    "    \n",
    "    # Optimize memory usage\n",
    "    df = optimize_dataframe(df)\n",
    "    \n",
    "    # Save historical data\n",
    "    df.to_parquet(config.HISTORICAL_DATA, index=False)\n",
    "    \n",
    "    return df, feature_cols\n",
    "\n",
    "\n",
    "# Execute data loading\n",
    "df, feature_cols = load_and_preprocess_data()\n",
    "\n",
    "print(f\"\\n✓ Data preprocessing complete!\")\n",
    "print(f\"  Final dataset size: {len(df):,} records\")\n",
    "print(f\"  Number of features: {len(feature_cols)}\")\n",
    "print(f\"  Feature columns: {feature_cols}\")\n",
    "print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nSample data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb93e1",
   "metadata": {},
   "source": [
    "## Event Log Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf22add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating event log chunks in poc_data\\event_log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating chunks: 100%|██████████| 20/20 [00:00<00:00, 20.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 20 event chunks\n",
      "\n",
      "✓ Event log created!\n",
      "  Number of chunks: 20\n",
      "  Chunk size: 100,000 records\n",
      "  Total columns: 9\n",
      "  Chunk files created: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_event_log(df):\n",
    "    # Save historical data\n",
    "    df.to_parquet(config.HISTORICAL_DATA, index=False)\n",
    "    \n",
    "    # Create log chunks\n",
    "    print(f\"Creating event log chunks in {config.LOG_DIR}\")\n",
    "    num_chunks = int(np.ceil(len(df) / config.CHUNK_SIZE))\n",
    "    \n",
    "    for i in tqdm(range(num_chunks), desc=\"Creating chunks\"):\n",
    "        start = i * config.CHUNK_SIZE\n",
    "        end = min((i+1) * config.CHUNK_SIZE, len(df))\n",
    "        chunk = df.iloc[start:end]\n",
    "        \n",
    "        # Save as parquet for efficiency\n",
    "        fname = config.LOG_DIR / f\"events_{i:05d}.parquet\"\n",
    "        chunk.to_parquet(fname, index=False)\n",
    "    \n",
    "    print(f\"✓ Created {num_chunks} event chunks\")\n",
    "    return num_chunks, df.columns.tolist()\n",
    "\n",
    "# Execute event log creation\n",
    "num_chunks, all_cols = create_event_log(df)\n",
    "\n",
    "print(f\"\\n✓ Event log created!\")\n",
    "print(f\"  Number of chunks: {num_chunks}\")\n",
    "print(f\"  Chunk size: {config.CHUNK_SIZE:,} records\")\n",
    "print(f\"  Total columns: {len(all_cols)}\")\n",
    "\n",
    "# Verify chunks were created\n",
    "chunk_files = list(config.LOG_DIR.glob(\"events_*.parquet\"))\n",
    "print(f\"  Chunk files created: {len(chunk_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd9ae5a",
   "metadata": {},
   "source": [
    "## Train Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5c589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MiniBatchKMeans with K=6...\n",
      "  Training in mini-batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|██████████| 40/40 [00:07<00:00,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved model to poc_data\\models\\kmeans_k6.joblib\n",
      "\n",
      "✓ Model training complete!\n",
      "  Model type: MiniBatchKMeans\n",
      "  Number of clusters: 6\n",
      "  Model saved to: poc_data\\models\\kmeans_k6.joblib\n",
      "  Inertia: 665685.12\n",
      "\n",
      "Cluster centers shape: (6, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_model(df, feature_cols, k=None, model_name=None):\n",
    "    k = k or config.INITIAL_K\n",
    "    model_name = model_name or f\"kmeans_k{k}.joblib\"\n",
    "    \n",
    "    print(f\"Training MiniBatchKMeans with K={k}...\")\n",
    "    model = MiniBatchKMeans(\n",
    "        n_clusters=k, \n",
    "        random_state=config.RANDOM_SEED,\n",
    "        batch_size=10000,  # Process in smaller batches\n",
    "        n_init=3\n",
    "    )\n",
    "    \n",
    "    # Fit in mini-batches for memory efficiency\n",
    "    print(\"  Training in mini-batches...\")\n",
    "    for i in tqdm(range(0, len(df), 50000), desc=\"Training batches\"):\n",
    "        batch = df[feature_cols].iloc[i:i+50000]\n",
    "        model.partial_fit(batch)\n",
    "    \n",
    "    model_path = config.MODEL_DIR / model_name\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"✓ Saved model to {model_path}\")\n",
    "    \n",
    "    return model, model_path\n",
    "\n",
    "# Train initial model\n",
    "initial_model, model_path = train_model(df, feature_cols)\n",
    "\n",
    "print(f\"\\n✓ Model training complete!\")\n",
    "print(f\"  Model type: MiniBatchKMeans\")\n",
    "print(f\"  Number of clusters: {config.INITIAL_K}\")\n",
    "print(f\"  Model saved to: {model_path}\")\n",
    "print(f\"  Inertia: {initial_model.inertia_:.2f}\")\n",
    "\n",
    "# Show cluster centers\n",
    "print(f\"\\nCluster centers shape: {initial_model.cluster_centers_.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a46add",
   "metadata": {},
   "source": [
    "## Initializing Feature Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b7ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Feature stores initialized\n",
      "✓ Feature stores initialized!\n",
      "  Online store: SQLite database at poc_data\\feature_store.db\n",
      "  Offline store: Parquet files in poc_data\\offline_features\n",
      "  Database tables: ['online_features']\n"
     ]
    }
   ],
   "source": [
    "def init_feature_stores(reset=True):    \n",
    "    # Handle existing database\n",
    "    if reset and config.SQLITE_DB.exists():\n",
    "        try:\n",
    "            config.SQLITE_DB.unlink()\n",
    "        except PermissionError:\n",
    "            print(\"[WARN] Database file is currently in use. Trying to close connections...\")\n",
    "            import gc\n",
    "            gc.collect()  # Force garbage collection of unused connections\n",
    "            time.sleep(0.1)\n",
    "            config.SQLITE_DB.unlink()\n",
    "    \n",
    "    # Create new DB\n",
    "    conn = sqlite3.connect(config.SQLITE_DB)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS online_features (\n",
    "        trip_id TEXT PRIMARY KEY,\n",
    "        cluster_id INTEGER,\n",
    "        last_updated TIMESTAMP,\n",
    "        trip_distance REAL,\n",
    "        total_amount REAL,\n",
    "        pickup_hour INTEGER,\n",
    "        pickup_day INTEGER,\n",
    "        trip_duration REAL,\n",
    "        passenger_count INTEGER\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    # Clean offline store\n",
    "    for f in config.OFFLINE_FEATURE_DIR.glob(\"*.parquet\"):\n",
    "        f.unlink()\n",
    "    \n",
    "    print(\"✓ Feature stores initialized\")\n",
    "\n",
    "# Initialize feature stores\n",
    "init_feature_stores(reset=True)\n",
    "\n",
    "print(f\"✓ Feature stores initialized!\")\n",
    "print(f\"  Online store: SQLite database at {config.SQLITE_DB}\")\n",
    "print(f\"  Offline store: Parquet files in {config.OFFLINE_FEATURE_DIR}\")\n",
    "\n",
    "# Verify database creation\n",
    "conn = sqlite3.connect(config.SQLITE_DB)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "conn.close()\n",
    "print(f\"  Database tables: {[table[0] for table in tables]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c8454",
   "metadata": {},
   "source": [
    "## Setting Up VectorDB (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1e9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Vector database initialized\n",
      "✓ Vector database setup complete!\n",
      "  Client: ChromaDB persistent client\n",
      "  Collection: trip_embeddings\n",
      "  Storage path: poc_data\\vector_db\n",
      "  Distance metric: Cosine\n"
     ]
    }
   ],
   "source": [
    "def setup_vector_db(reset=True):\n",
    "    chroma_client = chromadb.PersistentClient(\n",
    "        path=str(config.VECTOR_DB_DIR),\n",
    "        settings=Settings(allow_reset=reset)\n",
    "    )\n",
    "    \n",
    "    # Create collection\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=\"trip_embeddings\",\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Vector database initialized\")\n",
    "    return chroma_client, collection\n",
    "\n",
    "# Setup vector database\n",
    "chroma_client, collection = setup_vector_db(reset=True)\n",
    "\n",
    "print(f\"✓ Vector database setup complete!\")\n",
    "print(f\"  Client: ChromaDB persistent client\")\n",
    "print(f\"  Collection: trip_embeddings\")\n",
    "print(f\"  Storage path: {config.VECTOR_DB_DIR}\")\n",
    "print(f\"  Distance metric: Cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66fa6e",
   "metadata": {},
   "source": [
    "## Fitting Scaler and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584397fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting StandardScaler and PCA on historical data...\n",
      "✓ PCA and Scaler fitted on historical data\n",
      "  PCA components: 8\n",
      "  Explained variance ratio (first 5): [0.34288306 0.140263   0.12783096 0.12558373 0.11959544]\n",
      "  Cumulative variance explained: 1.000\n",
      "\n",
      "✓ Dimensionality reduction models ready!\n",
      "  Original features: 8\n",
      "  Reduced dimensions: 8\n",
      "  Variance preserved: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Global variables for PCA and Scaler\n",
    "pca_model = None\n",
    "scaler_model = None\n",
    "\n",
    "def fit_scaler_pca(df, feature_cols):\n",
    "    global pca_model, scaler_model\n",
    "    \n",
    "    print(\"Fitting StandardScaler and PCA on historical data...\")\n",
    "    \n",
    "    # Fit scaler\n",
    "    scaler_model = StandardScaler()\n",
    "    scaled_features = scaler_model.fit_transform(df[feature_cols])\n",
    "    \n",
    "    # Fit PCA\n",
    "    pca_model = PCA(n_components=min(config.EMBEDDING_DIM, len(feature_cols)))\n",
    "    pca_model.fit(scaled_features)\n",
    "    \n",
    "    explained_var_ratio = pca_model.explained_variance_ratio_\n",
    "    cumulative_var_ratio = np.cumsum(explained_var_ratio)\n",
    "    \n",
    "    print(\"✓ PCA and Scaler fitted on historical data\")\n",
    "    print(f\"  PCA components: {pca_model.n_components_}\")\n",
    "    print(f\"  Explained variance ratio (first 5): {explained_var_ratio[:5]}\")\n",
    "    print(f\"  Cumulative variance explained: {cumulative_var_ratio[-1]:.3f}\")\n",
    "    \n",
    "    return pca_model, scaler_model\n",
    "\n",
    "# Fit PCA and Scaler\n",
    "pca_fitted, scaler_fitted = fit_scaler_pca(df, feature_cols)\n",
    "\n",
    "print(f\"\\n✓ Dimensionality reduction models ready!\")\n",
    "print(f\"  Original features: {len(feature_cols)}\")\n",
    "print(f\"  Reduced dimensions: {pca_fitted.n_components_}\")\n",
    "print(f\"  Variance preserved: {np.sum(pca_fitted.explained_variance_ratio_):.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092be2c",
   "metadata": {},
   "source": [
    "## Upsert to Online Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708fbd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_to_online_store(engine, output_df):\n",
    "    try:\n",
    "        # Prepare records as dictionaries\n",
    "        records = output_df[[\n",
    "            'trip_id', 'cluster_id', 'trip_distance', 'total_amount',\n",
    "            'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count'\n",
    "        ]].copy()\n",
    "        \n",
    "        # Convert timestamp to string format for SQLite compatibility\n",
    "        records['last_updated'] = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Convert to list of dicts\n",
    "        records_list = records.to_dict(orient='records')\n",
    "\n",
    "        # Use SQLAlchemy bulk execute\n",
    "        with engine.begin() as conn:  # begin() handles commit automatically\n",
    "            conn.execute(\n",
    "                text('''\n",
    "                        INSERT OR REPLACE INTO online_features\n",
    "                        (trip_id, cluster_id, trip_distance, total_amount, pickup_hour, pickup_day, trip_duration, passenger_count, last_updated)\n",
    "                        VALUES (:trip_id, :cluster_id, :trip_distance, :total_amount, :pickup_hour, :pickup_day, :trip_duration, :passenger_count, :last_updated)\n",
    "                        '''), \n",
    "                        records_list\n",
    "                    )\n",
    "\n",
    "        print(f\"✓ Upserted {len(records_list)} rows into online_features\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error upserting to online store: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220a634",
   "metadata": {},
   "source": [
    "## Store Embedding in VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f159bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(collection, df, feature_cols, batch_size=5000):\n",
    "    global pca_model, scaler_model\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"[WARN] Empty DataFrame, skipping embedding storage\")\n",
    "        return 0\n",
    "\n",
    "    print(f\"  Preparing to store embeddings for {len(df)} rows\")\n",
    "\n",
    "    try:\n",
    "        # Ensure numeric features only\n",
    "        df_features = df[feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        \n",
    "        # Standardize and reduce dimensions\n",
    "        scaled_features = scaler_model.transform(df_features)\n",
    "        embeddings = pca_model.transform(scaled_features)\n",
    "        print(f\"  Embeddings created with shape: {embeddings.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR in embedding creation: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "    # Ensure processed_at exists\n",
    "    if 'processed_at' not in df.columns:\n",
    "        df['processed_at'] = datetime.now(timezone.utc)\n",
    "\n",
    "    # Store embeddings in batches\n",
    "    total_stored = 0\n",
    "    for i in range(0, len(embeddings), batch_size):\n",
    "        batch_end = min(i + batch_size, len(embeddings))\n",
    "        batch_ids = df['trip_id'].iloc[i:batch_end].astype(str).tolist()\n",
    "        batch_embeddings = embeddings[i:batch_end].tolist()\n",
    "        batch_metadatas = []\n",
    "        \n",
    "        for _, row in df.iloc[i:batch_end].iterrows():\n",
    "            try:\n",
    "                batch_metadatas.append({\n",
    "                    'cluster_id': int(row['cluster_id']),\n",
    "                    'trip_distance': float(row['trip_distance']),\n",
    "                    'total_amount': float(row['total_amount']),\n",
    "                    'pickup_hour': int(row['pickup_hour']),\n",
    "                    'trip_duration': float(row.get('trip_duration', 0)),\n",
    "                    'processed_at': row['processed_at'].isoformat() if hasattr(row['processed_at'], 'isoformat') else str(row['processed_at'])\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR creating metadata for row: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            collection.add(\n",
    "                ids=batch_ids,\n",
    "                embeddings=batch_embeddings,\n",
    "                metadatas=batch_metadatas\n",
    "            )\n",
    "            total_stored += len(batch_ids)\n",
    "            print(f\"  Added batch {i//batch_size + 1}, stored {len(batch_ids)} embeddings\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR adding batch to vector DB: {str(e)}\")\n",
    "            # Try to add without metadata first to isolate the issue\n",
    "            try:\n",
    "                collection.add(\n",
    "                    ids=batch_ids,\n",
    "                    embeddings=batch_embeddings\n",
    "                )\n",
    "                print(f\"  Successfully added batch without metadata\")\n",
    "                total_stored += len(batch_ids)\n",
    "            except Exception as e2:\n",
    "                print(f\"  ERROR adding batch without metadata: {str(e2)}\")\n",
    "                # Try to add just one item to see if there's a specific issue\n",
    "                try:\n",
    "                    collection.add(\n",
    "                        ids=[batch_ids[0]],\n",
    "                        embeddings=[batch_embeddings[0]]\n",
    "                    )\n",
    "                    print(f\"  Successfully added single item\")\n",
    "                    total_stored += 1\n",
    "                except Exception as e3:\n",
    "                    print(f\"  ERROR adding single item: {str(e3)}\")\n",
    "\n",
    "    print(f\"✓ Stored {total_stored} embeddings in vector DB\")\n",
    "    return total_stored\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d379628",
   "metadata": {},
   "source": [
    "## Process Event Stream and Populate Feature Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178918f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20 chunks...\n",
      "Feature columns expected: ['trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing events_00000.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00000_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   5%|▌         | 1/20 [00:19<06:01, 19.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00001.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00001_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  10%|█         | 2/20 [00:36<05:24, 18.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00002.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00002_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  15%|█▌        | 3/20 [00:53<05:00, 17.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00003.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00003_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  20%|██        | 4/20 [01:10<04:39, 17.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00004.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00004_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  25%|██▌       | 5/20 [01:27<04:18, 17.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00005.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00005_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  30%|███       | 6/20 [01:44<04:01, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00006.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00006_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  35%|███▌      | 7/20 [02:02<03:44, 17.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00007.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00007_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  40%|████      | 8/20 [02:19<03:27, 17.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00008.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00008_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  45%|████▌     | 9/20 [02:36<03:08, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00009.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00009_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  50%|█████     | 10/20 [02:54<02:53, 17.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00010.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00010_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  55%|█████▌    | 11/20 [03:11<02:34, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00011.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00011_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  60%|██████    | 12/20 [03:28<02:17, 17.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00012.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00012_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  65%|██████▌   | 13/20 [03:45<02:00, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00013.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00013_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  70%|███████   | 14/20 [04:02<01:43, 17.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00014.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00014_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  75%|███████▌  | 15/20 [04:20<01:27, 17.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00015.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00015_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  80%|████████  | 16/20 [04:38<01:10, 17.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00016.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00016_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  85%|████████▌ | 17/20 [04:56<00:52, 17.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00017.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00017_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  90%|█████████ | 18/20 [05:14<00:35, 17.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00018.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00018_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  95%|█████████▌| 19/20 [05:33<00:18, 18.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00019.parquet...\n",
      "  Chunk shape: (56509, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (56509, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00019_features.parquet\n",
      "✓ Upserted 56509 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 56509 rows\n",
      "  Embeddings created with shape: (56509, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 20/20 [05:44<00:00, 17.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 12, stored 1509 embeddings\n",
      "✓ Stored 56509 embeddings in vector DB\n",
      "  Stored 56509 embeddings\n",
      "✓ Stream processing complete. Total rows processed and stored: 1,956,509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB now contains 1956509 embeddings\n",
      "\n",
      "Checking vector database contents...\n",
      "Items in vector database: 1956509\n",
      "Sample IDs: ['trip_0', 'trip_1', 'trip_2', 'trip_3', 'trip_4']\n",
      "Sample metadata keys: ['processed_at', 'trip_distance', 'trip_duration', 'cluster_id', 'pickup_hour', 'total_amount']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1956509"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_stream_batch(model, feature_cols, collection):\n",
    "    engine = create_engine(f\"sqlite:///{config.SQLITE_DB}\")\n",
    "    processed_total = 0\n",
    "\n",
    "    chunk_files = sorted(config.LOG_DIR.glob(\"events_*.parquet\"))\n",
    "    if not chunk_files:\n",
    "        print(\"[WARN] No event chunks found to process\")\n",
    "        return 0\n",
    "\n",
    "    print(f\"Processing {len(chunk_files)} chunks...\")\n",
    "    print(f\"Feature columns expected: {feature_cols}\")\n",
    "\n",
    "    for fname in tqdm(chunk_files, desc=\"Processing chunks\"):\n",
    "        try:\n",
    "            print(f\"\\nProcessing {fname.name}...\")\n",
    "            chunk = pd.read_parquet(fname)\n",
    "            \n",
    "            if chunk.empty:\n",
    "                print(f\"  Chunk is empty, skipping\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"  Chunk shape: {chunk.shape}\")\n",
    "            print(f\"  Chunk columns: {chunk.columns.tolist()}\")\n",
    "            \n",
    "            # Check if all required feature columns exist\n",
    "            missing_cols = [col for col in feature_cols if col not in chunk.columns]\n",
    "            if missing_cols:\n",
    "                print(f\"  Missing columns: {missing_cols}\")\n",
    "                continue\n",
    "                \n",
    "            # Check for NaN values in feature columns\n",
    "            nan_counts = chunk[feature_cols].isna().sum()\n",
    "            if nan_counts.any():\n",
    "                print(f\"  NaN values found: {nan_counts[nan_counts > 0].to_dict()}\")\n",
    "                \n",
    "            # Fill numeric features\n",
    "            chunk_filled = chunk[feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "            print(f\"  Filled chunk shape: {chunk_filled.shape}\")\n",
    "            \n",
    "            # Check if all values are numeric after conversion\n",
    "            non_numeric_cols = []\n",
    "            for col in chunk_filled.columns:\n",
    "                if not pd.api.types.is_numeric_dtype(chunk_filled[col]):\n",
    "                    non_numeric_cols.append(col)\n",
    "                    \n",
    "            if non_numeric_cols:\n",
    "                print(f\"  Non-numeric columns after conversion: {non_numeric_cols}\")\n",
    "                continue\n",
    "                \n",
    "            # Predict cluster IDs\n",
    "            cluster_ids = model.predict(chunk_filled)\n",
    "            chunk['cluster_id'] = cluster_ids\n",
    "            chunk['processed_at'] = datetime.now(timezone.utc)\n",
    "            \n",
    "            print(f\"  Cluster IDs assigned: {len(np.unique(cluster_ids))} unique clusters\")\n",
    "            \n",
    "            # Save offline features\n",
    "            offline_path = config.OFFLINE_FEATURE_DIR / f\"{fname.stem}_features.parquet\"\n",
    "            chunk.to_parquet(offline_path, index=False)\n",
    "            print(f\"  Offline features saved to {offline_path}\")\n",
    "            \n",
    "            # Upsert to SQLite online store\n",
    "            upsert_success = upsert_to_online_store(engine, chunk)\n",
    "            if not upsert_success:\n",
    "                print(f\"  Failed to upsert to online store\")\n",
    "            \n",
    "            # Store embeddings in vector DB\n",
    "            print(f\"  Storing embeddings...\")\n",
    "            stored_count = store_embeddings(collection, chunk, feature_cols)\n",
    "            processed_total += stored_count\n",
    "            \n",
    "            if stored_count == 0:\n",
    "                print(f\"  WARNING: No embeddings stored for this chunk\")\n",
    "            else:\n",
    "                print(f\"  Stored {stored_count} embeddings\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR processing chunk: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    print(f\"✓ Stream processing complete. Total rows processed and stored: {processed_total:,}\")\n",
    "    \n",
    "    # Verify vector DB has content\n",
    "    db_count = collection.count()\n",
    "    print(f\"Vector DB now contains {db_count} embeddings\")\n",
    "    \n",
    "    return processed_total\n",
    "\n",
    "\n",
    "def check_vector_db_contents(collection):\n",
    "    print(\"\\nChecking vector database contents...\")\n",
    "    \n",
    "    # Count items in collection\n",
    "    count = collection.count()\n",
    "    print(f\"Items in vector database: {count}\")\n",
    "    \n",
    "    if count > 0:\n",
    "        # Get a sample of items\n",
    "        sample = collection.get(limit=min(5, count))\n",
    "        print(f\"Sample IDs: {sample['ids']}\")\n",
    "        if sample['metadatas']:\n",
    "            print(f\"Sample metadata keys: {list(sample['metadatas'][0].keys())}\")\n",
    "    else:\n",
    "        print(\"Vector database is empty\")\n",
    "        \n",
    "    return count\n",
    "processed_count = process_stream_batch(initial_model, feature_cols, collection)\n",
    "check_vector_db_contents(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ed690",
   "metadata": {},
   "source": [
    "## Checking VectorDB Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077848f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking vector database contents...\n",
      "Items in vector database: 1956509\n",
      "Sample IDs: ['trip_0', 'trip_1', 'trip_2', 'trip_3', 'trip_4']\n",
      "Sample metadata keys: ['pickup_hour', 'trip_duration', 'processed_at', 'total_amount', 'cluster_id', 'trip_distance']\n"
     ]
    }
   ],
   "source": [
    "def check_vector_db_contents(collection):\n",
    "    print(\"\\nChecking vector database contents...\")\n",
    "    \n",
    "    # Count items in collection\n",
    "    count = collection.count()\n",
    "    print(f\"Items in vector database: {count}\")\n",
    "    \n",
    "    if count > 0:\n",
    "        # Get a sample of items\n",
    "        sample = collection.get(limit=min(5, count))\n",
    "        print(f\"Sample IDs: {sample['ids']}\")\n",
    "        if sample['metadatas']:\n",
    "            print(f\"Sample metadata keys: {list(sample['metadatas'][0].keys())}\")\n",
    "    else:\n",
    "        print(\"Vector database is empty\")\n",
    "        \n",
    "    return count\n",
    "\n",
    "# Call this after processing\n",
    "db_count = check_vector_db_contents(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa6f471a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First chunk rows: 100000\n",
      "  trip_id  trip_distance  total_amount  pickup_hour  pickup_day  \\\n",
      "0  trip_0       0.400000     10.800000           15           0   \n",
      "1  trip_1       0.600000     12.950000           21           1   \n",
      "2  trip_2       2.100000     29.900000           18           2   \n",
      "3  trip_3      16.299999     96.650002            9           3   \n",
      "4  trip_4       1.740000     22.000000           17           4   \n",
      "\n",
      "   trip_duration  passenger_count  pickup_location  dropoff_location  \n",
      "0       4.266667              1.0                7                 7  \n",
      "1       5.066667              1.0                6                 6  \n",
      "2      22.549999              1.0                3                 6  \n",
      "3      36.849998              3.0                2                 2  \n",
      "4      14.500000              1.0                3                 1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_files = sorted(config.LOG_DIR.glob(\"events_*.parquet\"))\n",
    "if len(chunk_files) == 0:\n",
    "    print(\"No chunk files found!\")\n",
    "else:\n",
    "    chunk = pd.read_parquet(chunk_files[0])\n",
    "    print(f\"First chunk rows: {len(chunk)}\")\n",
    "    print(chunk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113086c",
   "metadata": {},
   "source": [
    "## Compute AI Metrics per Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9cf377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_similarity_search(collection, query_trip_id, n_results=5):\n",
    "    # Get the query embedding\n",
    "    result = collection.get(ids=[query_trip_id], include=['embeddings'])\n",
    "    \n",
    "    if not result['ids']:\n",
    "        print(f\"Trip ID {query_trip_id} not found in vector DB\")\n",
    "        return None\n",
    "    \n",
    "    query_embedding = result['embeddings'][0]\n",
    "    \n",
    "    # Search for similar trips\n",
    "    similar = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results + 1,  # +1 to exclude the query itself\n",
    "        include=['metadatas', 'distances']\n",
    "    )\n",
    "    \n",
    "    # Filter out the query trip itself\n",
    "    similar_trips = []\n",
    "    for i, trip_id in enumerate(similar['ids'][0]):\n",
    "        if trip_id != query_trip_id:\n",
    "            similar_trips.append({\n",
    "                'trip_id': trip_id,\n",
    "                'distance': similar['distances'][0][i],\n",
    "                'metadata': similar['metadatas'][0][i]\n",
    "            })\n",
    "    \n",
    "    return similar_trips[:n_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c81ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing AI-enhanced metrics (fast mode)...\n",
      "  Processing 5,000 sample records from vector database...\n",
      "  ✓ Cluster sizes computed\n",
      "  Skipping intra-cluster distance calculation for speed...\n",
      "  Computing anomaly scores...\n",
      "  ✓ Anomaly scores computed\n",
      "  Computing silhouette scores...\n",
      "  ✓ Silhouette score computed: 0.052\n",
      "  Computing business metrics...\n",
      "  ✓ Business metrics computed\n",
      "✓ AI metrics computed and saved to poc_data\\ai_metrics.parquet\n",
      "Demonstrating vector similarity search for trip: trip_0\n",
      "\n",
      "✓ Vector similarity search complete!\n",
      "  Query trip: trip_0\n",
      "  Similar trips found:\n",
      "    1. trip_97502 (distance: 0.0001)\n",
      "       Cluster: 4, Distance: 0.50mi\n",
      "       Amount: $10.90, Hour: 15\n",
      "    2. trip_1615648 (distance: 0.0002)\n",
      "       Cluster: 4, Distance: 0.33mi\n",
      "       Amount: $9.80, Hour: 15\n",
      "    3. trip_377180 (distance: 0.0002)\n",
      "       Cluster: 4, Distance: 0.41mi\n",
      "       Amount: $11.76, Hour: 15\n",
      "    4. trip_829681 (distance: 0.0003)\n",
      "       Cluster: 4, Distance: 0.50mi\n",
      "       Amount: $11.76, Hour: 15\n",
      "    5. trip_525678 (distance: 0.0004)\n",
      "       Cluster: 4, Distance: 0.60mi\n",
      "       Amount: $9.80, Hour: 15\n"
     ]
    }
   ],
   "source": [
    "def compute_ai_metrics_fast(collection, feature_cols, sample_size=5000):\n",
    "    print(\"Computing AI-enhanced metrics (fast mode)...\")\n",
    "    \n",
    "    # Get a sample of embeddings and metadata from vector DB\n",
    "    results = collection.get(limit=sample_size, include=['embeddings', 'metadatas'])\n",
    "    \n",
    "    if not results['ids']:\n",
    "        print(\"No data in vector database\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Processing {len(results['ids']):,} sample records from vector database...\")\n",
    "    \n",
    "    # Extract metadata into a DataFrame\n",
    "    metadata_list = []\n",
    "    for i, metadata in enumerate(results['metadatas']):\n",
    "        metadata_list.append({\n",
    "            'trip_id': results['ids'][i],\n",
    "            'cluster_id': metadata['cluster_id'],\n",
    "            'trip_distance': metadata['trip_distance'],\n",
    "            'total_amount': metadata['total_amount'],\n",
    "            'pickup_hour': metadata['pickup_hour'],\n",
    "            'trip_duration': metadata.get('trip_duration', 30)\n",
    "        })\n",
    "    \n",
    "    emb_df = pd.DataFrame(metadata_list)\n",
    "    \n",
    "    # Get embeddings as a separate array\n",
    "    embeddings = np.array(results['embeddings'])\n",
    "    \n",
    "    # Calculate cluster sizes\n",
    "    cluster_sizes = emb_df.groupby('cluster_id').size()\n",
    "    emb_df['cluster_size'] = emb_df['cluster_id'].map(cluster_sizes)\n",
    "    print(f\"  ✓ Cluster sizes computed\")\n",
    "    \n",
    "    # Skip intra-cluster distance calculation for speed\n",
    "    print(\"  Skipping intra-cluster distance calculation for speed...\")\n",
    "    emb_df['intra_cluster_distance'] = 0  # Placeholder\n",
    "    \n",
    "    # Calculate anomaly score\n",
    "    print(\"  Computing anomaly scores...\")\n",
    "    iso_forest = IsolationForest(\n",
    "        contamination=0.1, \n",
    "        random_state=config.RANDOM_SEED, \n",
    "        n_estimators=50,  # Fewer trees for speed\n",
    "        max_samples=min(256, len(embeddings))  # Limit sample size\n",
    "    )\n",
    "    emb_df['anomaly_score'] = iso_forest.fit_predict(embeddings)\n",
    "    print(f\"  ✓ Anomaly scores computed\")\n",
    "    \n",
    "    # Calculate silhouette score (on a smaller sample if needed)\n",
    "    print(\"  Computing silhouette scores...\")\n",
    "    silhouette_sample_size = min(1000, len(embeddings))\n",
    "    if len(embeddings) > silhouette_sample_size:\n",
    "        sample_indices = np.random.choice(len(embeddings), silhouette_sample_size, replace=False)\n",
    "        sample_embeddings = embeddings[sample_indices]\n",
    "        sample_clusters = emb_df['cluster_id'].iloc[sample_indices].values\n",
    "    else:\n",
    "        sample_embeddings = embeddings\n",
    "        sample_clusters = emb_df['cluster_id'].values\n",
    "    \n",
    "    silhouette_avg = silhouette_score(sample_embeddings, sample_clusters)\n",
    "    emb_df['silhouette_score'] = silhouette_avg\n",
    "    print(f\"  ✓ Silhouette score computed: {silhouette_avg:.3f}\")\n",
    "    \n",
    "    # Business metrics\n",
    "    print(\"  Computing business metrics...\")\n",
    "    emb_df['revenue_per_minute'] = emb_df['total_amount'] / (emb_df['trip_duration'] + 1)\n",
    "    emb_df['peak_hour'] = emb_df['pickup_hour'].isin([7, 8, 17, 18, 19]).astype(int)\n",
    "    print(f\"  ✓ Business metrics computed\")\n",
    "    \n",
    "    # Save AI metrics\n",
    "    ai_metrics = emb_df[[\n",
    "        'trip_id', 'cluster_id', 'cluster_size', 'intra_cluster_distance',\n",
    "        'anomaly_score', 'silhouette_score', 'revenue_per_minute', 'peak_hour'\n",
    "    ]]\n",
    "    \n",
    "    ai_metrics.to_parquet(config.AI_METRICS_FILE, index=False)\n",
    "    print(f\"✓ AI metrics computed and saved to {config.AI_METRICS_FILE}\")\n",
    "    \n",
    "    return ai_metrics\n",
    "\n",
    "# Use the faster version\n",
    "ai_metrics = compute_ai_metrics_fast(collection, feature_cols, sample_size=5000)\n",
    "\n",
    "# Demonstrate vector similarity search\n",
    "if ai_metrics is not None and len(ai_metrics) > 0:\n",
    "    sample_trip = ai_metrics.iloc[0]['trip_id']\n",
    "    print(f\"Demonstrating vector similarity search for trip: {sample_trip}\")\n",
    "    \n",
    "    similar_trips = vector_similarity_search(collection, sample_trip, n_results=5)\n",
    "    \n",
    "    if similar_trips:\n",
    "        print(f\"\\n✓ Vector similarity search complete!\")\n",
    "        print(f\"  Query trip: {sample_trip}\")\n",
    "        print(f\"  Similar trips found:\")\n",
    "        \n",
    "        for i, trip in enumerate(similar_trips, 1):\n",
    "            metadata = trip['metadata']\n",
    "            print(f\"    {i}. {trip['trip_id']} (distance: {trip['distance']:.4f})\")\n",
    "            print(f\"       Cluster: {metadata['cluster_id']}, Distance: {metadata['trip_distance']:.2f}mi\")\n",
    "            print(f\"       Amount: ${metadata['total_amount']:.2f}, Hour: {metadata['pickup_hour']}\")\n",
    "    else:\n",
    "        print(\"No similar trips found.\")\n",
    "else:\n",
    "    print(\"No AI metrics computed, skipping vector search demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff012a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking vector database contents...\n",
      "Items in vector database: 1956509\n",
      "Sample IDs: ['trip_0', 'trip_1', 'trip_2', 'trip_3', 'trip_4']\n",
      "Sample metadata keys: ['pickup_hour', 'total_amount', 'trip_duration', 'processed_at', 'cluster_id', 'trip_distance']\n"
     ]
    }
   ],
   "source": [
    "def check_vector_db_contents(collection):\n",
    "    print(\"\\nChecking vector database contents...\")\n",
    "    \n",
    "    # Count items in collection\n",
    "    count = collection.count()\n",
    "    print(f\"Items in vector database: {count}\")\n",
    "    \n",
    "    if count > 0:\n",
    "        # Get a sample of items\n",
    "        sample = collection.get(limit=min(5, count))\n",
    "        print(f\"Sample IDs: {sample['ids']}\")\n",
    "        if sample['metadatas']:\n",
    "            print(f\"Sample metadata keys: {list(sample['metadatas'][0].keys())}\")\n",
    "    else:\n",
    "        print(\"Vector database is empty\")\n",
    "        \n",
    "    return count\n",
    "\n",
    "# Call this after processing\n",
    "db_count = check_vector_db_contents(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03235b9",
   "metadata": {},
   "source": [
    "## Performance Comparison: Pre-computed and On-The Fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdad0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "\n",
    "def benchmark_performance(func, *args, verbose=True, **kwargs):\n",
    "    if verbose:\n",
    "        print(f\"\\nBenchmarking: {func.__name__} ...\")\n",
    "    \n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    result = func(*args, **kwargs)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    \n",
    "    memory_peak = peak / 1024**2  # Convert to MB\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"✓ Time taken: {elapsed_time:.2f} sec | Peak memory: {memory_peak:.2f} MB\")\n",
    "    \n",
    "    return result, elapsed_time, memory_peak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d5e2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_precomputed_features():\n",
    "    offline_files = sorted(config.OFFLINE_FEATURE_DIR.glob(\"*.parquet\"))\n",
    "    total_processed = 0\n",
    "    \n",
    "    for file in offline_files:\n",
    "        df_chunk = pd.read_parquet(file)\n",
    "        # Assign clusters (precomputed)\n",
    "        cluster_ids = initial_model.predict(df_chunk[feature_cols])\n",
    "        df_chunk['cluster_id'] = cluster_ids\n",
    "        df_chunk['processed_at'] = datetime.now(timezone.utc)\n",
    "        \n",
    "        # Upsert and store embeddings\n",
    "        engine = create_engine(f\"sqlite:///{config.SQLITE_DB}\")\n",
    "        upsert_to_online_store(engine, df_chunk)\n",
    "        store_embeddings(collection, df_chunk, feature_cols)\n",
    "        total_processed += len(df_chunk)\n",
    "    \n",
    "    return total_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "068ffcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_on_the_fly():\n",
    "    total_processed = process_stream_batch(initial_model, feature_cols, collection)\n",
    "    return total_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d0af7a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking: process_precomputed_features ...\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n",
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "✓ Upserted 56509 rows into online_features\n",
      "  Preparing to store embeddings for 56509 rows\n",
      "  Embeddings created with shape: (56509, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 1509 embeddings\n",
      "✓ Stored 56509 embeddings in vector DB\n",
      "✓ Time taken: 1241.88 sec | Peak memory: 85.76 MB\n",
      "\n",
      "Benchmarking: process_on_the_fly ...\n",
      "Processing 20 chunks...\n",
      "Feature columns expected: ['trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing events_00000.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00000_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   5%|▌         | 1/20 [01:08<21:35, 68.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00001.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00001_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  10%|█         | 2/20 [02:40<24:39, 82.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00002.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00002_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  15%|█▌        | 3/20 [03:50<21:47, 76.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00003.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00003_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  20%|██        | 4/20 [04:58<19:29, 73.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00004.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00004_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  25%|██▌       | 5/20 [06:15<18:38, 74.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00005.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00005_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  30%|███       | 6/20 [07:26<17:09, 73.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00006.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00006_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  35%|███▌      | 7/20 [08:29<15:09, 69.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00007.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00007_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  40%|████      | 8/20 [09:42<14:11, 70.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00008.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00008_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  45%|████▌     | 9/20 [11:12<14:07, 77.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00009.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00009_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  50%|█████     | 10/20 [12:44<13:35, 81.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00010.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00010_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  55%|█████▌    | 11/20 [13:58<11:53, 79.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00011.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00011_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  60%|██████    | 12/20 [15:15<10:28, 78.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00012.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00012_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  65%|██████▌   | 13/20 [16:18<08:35, 73.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00013.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00013_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  70%|███████   | 14/20 [17:23<07:06, 71.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00014.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00014_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  75%|███████▌  | 15/20 [18:30<05:50, 70.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00015.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00015_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  80%|████████  | 16/20 [19:44<04:44, 71.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00016.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00016_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  85%|████████▌ | 17/20 [20:51<03:29, 69.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00017.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00017_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  90%|█████████ | 18/20 [21:48<02:12, 66.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00018.parquet...\n",
      "  Chunk shape: (100000, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (100000, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00018_features.parquet\n",
      "✓ Upserted 100000 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 100000 rows\n",
      "  Embeddings created with shape: (100000, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n",
      "  Added batch 12, stored 5000 embeddings\n",
      "  Added batch 13, stored 5000 embeddings\n",
      "  Added batch 14, stored 5000 embeddings\n",
      "  Added batch 15, stored 5000 embeddings\n",
      "  Added batch 16, stored 5000 embeddings\n",
      "  Added batch 17, stored 5000 embeddings\n",
      "  Added batch 18, stored 5000 embeddings\n",
      "  Added batch 19, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  95%|█████████▌| 19/20 [22:50<01:04, 64.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 20, stored 5000 embeddings\n",
      "✓ Stored 100000 embeddings in vector DB\n",
      "  Stored 100000 embeddings\n",
      "\n",
      "Processing events_00019.parquet...\n",
      "  Chunk shape: (56509, 9)\n",
      "  Chunk columns: ['trip_id', 'trip_distance', 'total_amount', 'pickup_hour', 'pickup_day', 'trip_duration', 'passenger_count', 'pickup_location', 'dropoff_location']\n",
      "  Filled chunk shape: (56509, 8)\n",
      "  Cluster IDs assigned: 6 unique clusters\n",
      "  Offline features saved to poc_data\\offline_features\\events_00019_features.parquet\n",
      "✓ Upserted 56509 rows into online_features\n",
      "  Storing embeddings...\n",
      "  Preparing to store embeddings for 56509 rows\n",
      "  Embeddings created with shape: (56509, 8)\n",
      "  Added batch 1, stored 5000 embeddings\n",
      "  Added batch 2, stored 5000 embeddings\n",
      "  Added batch 3, stored 5000 embeddings\n",
      "  Added batch 4, stored 5000 embeddings\n",
      "  Added batch 5, stored 5000 embeddings\n",
      "  Added batch 6, stored 5000 embeddings\n",
      "  Added batch 7, stored 5000 embeddings\n",
      "  Added batch 8, stored 5000 embeddings\n",
      "  Added batch 9, stored 5000 embeddings\n",
      "  Added batch 10, stored 5000 embeddings\n",
      "  Added batch 11, stored 5000 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 20/20 [23:33<00:00, 70.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added batch 12, stored 1509 embeddings\n",
      "✓ Stored 56509 embeddings in vector DB\n",
      "  Stored 56509 embeddings\n",
      "✓ Stream processing complete. Total rows processed and stored: 1,956,509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB now contains 1956509 embeddings\n",
      "✓ Time taken: 1413.97 sec | Peak memory: 88.32 MB\n",
      "\n",
      "=== Performance Comparison ===\n",
      "Precomputed: 1,956,509 rows, Time: 1241.88s, Memory: 85.76 MB\n",
      "On-the-fly: 1,956,509 rows, Time: 1413.97s, Memory: 88.32 MB\n"
     ]
    }
   ],
   "source": [
    "# Precomputed\n",
    "precomputed_result, precomputed_time, precomputed_mem = benchmark_performance(process_precomputed_features)\n",
    "\n",
    "# On-the-fly\n",
    "onfly_result, onfly_time, onfly_mem = benchmark_performance(process_on_the_fly)\n",
    "\n",
    "print(\"\\n=== Performance Comparison ===\")\n",
    "print(f\"Precomputed: {precomputed_result:,} rows, Time: {precomputed_time:.2f}s, Memory: {precomputed_mem:.2f} MB\")\n",
    "print(f\"On-the-fly: {onfly_result:,} rows, Time: {onfly_time:.2f}s, Memory: {onfly_mem:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1d9dc6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing AI-enhanced metrics (fast mode)...\n",
      "  Processing 5,000 sample records from vector database...\n",
      "  ✓ Cluster sizes computed\n",
      "  Skipping intra-cluster distance calculation for speed...\n",
      "  Computing anomaly scores...\n",
      "  ✓ Anomaly scores computed\n",
      "  Computing silhouette scores...\n",
      "  ✓ Silhouette score computed: 0.054\n",
      "  Computing business metrics...\n",
      "  ✓ Business metrics computed\n",
      "✓ AI metrics computed and saved to poc_data\\ai_metrics.parquet\n",
      "Computing AI-enhanced metrics (fast mode)...\n",
      "  Processing 5,000 sample records from vector database...\n",
      "  ✓ Cluster sizes computed\n",
      "  Skipping intra-cluster distance calculation for speed...\n",
      "  Computing anomaly scores...\n",
      "  ✓ Anomaly scores computed\n",
      "  Computing silhouette scores...\n",
      "  ✓ Silhouette score computed: 0.048\n",
      "  Computing business metrics...\n",
      "  ✓ Business metrics computed\n",
      "✓ AI metrics computed and saved to poc_data\\ai_metrics.parquet\n",
      "Silhouette score difference: 0.006697334928993763\n"
     ]
    }
   ],
   "source": [
    "ai_metrics_precomputed = compute_ai_metrics_fast(collection, feature_cols)\n",
    "# Reset DB if needed, then run on-the-fly metrics\n",
    "ai_metrics_onfly = compute_ai_metrics_fast(collection, feature_cols)\n",
    "print(\"Silhouette score difference:\", \n",
    "      ai_metrics_precomputed['silhouette_score'].mean() - ai_metrics_onfly['silhouette_score'].mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
